import os
import torch
import logging
import sqlite3
import gradio as gr
from datetime import datetime
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from ollama import chat

# ---------------------- Logging Setup ----------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ---------------------- Database Setup ----------------------
def init_db():
    conn = sqlite3.connect("questions.db")
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS questions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            question TEXT NOT NULL,
            answer TEXT NOT NULL,
            timestamp TEXT NOT NULL
        )
    """)
    conn.commit()
    conn.close()
    logging.info("üì¶ Database initialized successfully.")

def save_question_to_db(question, answer):
    conn = sqlite3.connect("questions.db")
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO questions (question, answer, timestamp)
        VALUES (?, ?, ?)
    """, (question, answer, datetime.now().isoformat()))
    conn.commit()
    conn.close()
    logging.info("‚úÖ Saved question to database.")

# ---------------------- RAG Pipeline ----------------------
# 1. Load document
logging.info("üìÑ Loading document...")
loader = UnstructuredFileLoader("Loan_Features.docx")
docs = loader.load()

if not docs or not docs[0].page_content.strip():
    raise ValueError("‚ùå ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

# 2. Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(docs)
logging.info(f"‚úÖ Document split into {len(chunks)} chunks")

# 3. Load Embedding Model
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)
logging.info("üí° Embedding model loaded")

# 4. Create Vector Store
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="chroma_db"
)
logging.info("üìö Vector store created successfully")

# 5. Retrieval Function
def retrieve(query: str):
    return vectorstore.similarity_search(query, k=3)

# 6. Answer Generation with Ollama
def generate_answer(user_query: str):
    retrieved_docs = retrieve(user_query)
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    messages = [
        {
            "role": "system",
            "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô ai ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡∏Å‡∏¢‡∏® ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‡∏Ñ‡∏£‡∏±‡∏ö "
        },
        {
            "role": "user",
            "content": (
                f"Context:\n{context}\n\n"
                "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n"
                "Q: ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà‡∏ï‡πà‡∏≠‡∏õ‡∏µ?\n"
                "A: ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 360,000 ‡∏ö‡∏≤‡∏ó‡∏ï‡πà‡∏≠‡∏õ‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö\n"
                "Q: ‡∏Ñ‡∏ô‡∏≠‡∏≤‡∏¢‡∏∏ 33 ‡∏õ‡∏µ ‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?\n"
                "A: ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà 4 (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏≠‡∏≤‡∏¢‡∏∏‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 35 ‡∏õ‡∏µ)\n\n"
                f"Q: {user_query}\nA:"
            )
        }
    ]
    response = chat(model="llama3.2:latest", messages=messages)
    answer = response["message"]["content"]
    save_question_to_db(user_query, answer)
    return answer

# ---------------------- Gradio UI ----------------------
init_db()

demo = gr.Interface(
    fn=generate_answer,
    inputs=gr.Textbox(label="‚ùì ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡∏Å‡∏¢‡∏®"),
    outputs=gr.Textbox(label="‚úÖ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö"),
    title="üìÑ RAG Chatbot ‡∏Å‡∏¢‡∏®",
    description="‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ `Loan_Features.docx` ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ RAG + Ollama",
    allow_flagging="never"
)

demo.launch()