import os
import sqlite3
import logging
import time
from datetime import datetime

import streamlit as st
from dotenv import load_dotenv

from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_ollama import OllamaLLM, OllamaEmbeddings

# ---------------------- Load Environment ----------------------
load_dotenv()
OLLAMA_URL = os.getenv("OLLAMA_BASE_URL")

# ---------------------- Logging ----------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ---------------------- Database Functions ----------------------
DB_PATH = "questions.db"

def init_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS user_messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_message TEXT NOT NULL,
            answer TEXT NOT NULL,
            timestamp TEXT NOT NULL
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS retrieved_chunks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_message_id INTEGER NOT NULL,
            chunk_text TEXT NOT NULL,
            source TEXT,
            page_number INTEGER,
            FOREIGN KEY(user_message_id) REFERENCES user_messages(id)
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS llm_metrics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_message_id INTEGER NOT NULL,
            prompt_tokens INTEGER,
            response_tokens INTEGER,
            response_time REAL,
            timestamp TEXT NOT NULL,
            FOREIGN KEY(user_message_id) REFERENCES user_messages(id)
        )
    """)
    c.execute("""
        CREATE TABLE IF NOT EXISTS feedback (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_message_id INTEGER,
            satisfaction TEXT NOT NULL,
            feedback_text TEXT,
            timestamp TEXT NOT NULL,
            FOREIGN KEY(user_message_id) REFERENCES user_messages(id)
        )
    """)
    conn.commit()
    conn.close()
    logging.info("üì¶ Database initialized successfully.")

def save_user_message(user_message, answer):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("""
        INSERT INTO user_messages (user_message, answer, timestamp)
        VALUES (?, ?, ?)
    """, (user_message, answer, datetime.now().isoformat()))
    message_id = c.lastrowid
    conn.commit()
    conn.close()
    logging.info("‚úÖ Saved user message & answer to DB")
    return message_id

def save_retrieved_chunks(user_message_id, chunks):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    for chunk in chunks:
        page_number = chunk.metadata.get("page_number", 0)
        c.execute("""
            INSERT INTO retrieved_chunks (user_message_id, chunk_text, source, page_number)
            VALUES (?, ?, ?, ?)
        """, (user_message_id, chunk.page_content, chunk.metadata.get("source"), page_number))
    conn.commit()
    conn.close()
    logging.info(f"üìù Saved {len(chunks)} chunks for user_message_id {user_message_id}")

def save_llm_metrics(user_message_id, prompt_tokens, response_tokens, response_time):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("""
        INSERT INTO llm_metrics (user_message_id, prompt_tokens, response_tokens, response_time, timestamp)
        VALUES (?, ?, ?, ?, ?)
    """, (user_message_id, prompt_tokens, response_tokens, response_time, datetime.now().isoformat()))
    conn.commit()
    conn.close()
    logging.info(f"üìä Saved LLM metrics for user_message_id {user_message_id}")

def save_feedback(user_message_id, satisfaction, feedback_text):
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("""
        INSERT INTO feedback (user_message_id, satisfaction, feedback_text, timestamp)
        VALUES (?, ?, ?, ?)
    """, (user_message_id, satisfaction, feedback_text, datetime.now().isoformat()))
    conn.commit()
    conn.close()
    logging.info(f"üí¨ Saved feedback for message {user_message_id}: {satisfaction}")

def count_tokens(text: str) -> int:
    return len(text.split())

# ---------------------- Load CSS ----------------------
def load_css():
    try:
        with open('styles.css', 'r', encoding='utf-8') as f:
            css = f.read()
        st.markdown(f'<style>{css}</style>', unsafe_allow_html=True)
    except FileNotFoundError:
        st.warning("styles.css not found. The app will run with default styling.")

# ---------------------- Streamlit Setup ----------------------
st.set_page_config(
    page_title="RAG Chatbot ‡∏Å‡∏¢‡∏®", 
    page_icon="üéì",
    layout="wide",
    initial_sidebar_state="expanded"
)

load_css()

# ---------------------- Header ----------------------
st.markdown("""
    <div class="chat-header">
        <h1 style="margin:0; font-size: 2.5rem;">üéì RAG Chatbot ‡∏Å‡∏¢‡∏®</h1>
        <p style="margin:0.5rem 0 0 0; font-size: 1rem;">
            ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏¢‡∏®
        </p>
    </div>
""", unsafe_allow_html=True)

# ---------------------- Sidebar ----------------------
with st.sidebar:
    st.markdown("### üìö ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏ö")
    st.markdown("""
    <div class='sidebar-info-box'>
        <p style='margin: 0; color: #374151;'><strong>ü§ñ Model:</strong> Llama 3.2</p>
        <p style='margin: 0.5rem 0; color: #374151;'><strong>üìÑ Embeddings:</strong> BGE-M3</p>
        <p style='margin: 0; color: #374151;'><strong>üîç Retrieved Chunks:</strong> 3</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    st.markdown("### üí° ‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    st.markdown("""
    <div style='color: #374151; line-height: 1.8;'>
        <ol style='color: #374151; padding-left: 1.2rem;'>
            <li style='margin: 0.5rem 0;'>‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡πÅ‡∏ä‡∏ó</li>
            <li style='margin: 0.5rem 0;'>‡∏£‡∏≠‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°</li>
            <li style='margin: 0.5rem 0;'>‡∏Å‡∏î "üõë ‡∏à‡∏ö‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤" ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏ö</li>
            <li style='margin: 0.5rem 0;'>‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à</li>
            <li style='margin: 0.5rem 0;'>‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏ä‡∏ó‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ</li>
        </ol>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    if st.session_state.get("messages"):
        st.markdown("### üí¨ ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤")
        total_msgs = len([m for m in st.session_state.messages if m["role"] == "user"])
        st.markdown(f"""
        <div class='sidebar-info-box' style='text-align: center;'>
            <p style='margin: 0; color: #6b7280; font-size: 0.9rem;'>‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°</p>
            <h2 style='margin: 0.5rem 0; color: #3b82f6;'>{total_msgs}</h2>
        </div>
        """, unsafe_allow_html=True)

# ---------------------- Initialize ----------------------
init_db()
if "messages" not in st.session_state:
    st.session_state.messages = []

# ---------------------- Load Document ----------------------
doc_path = "Loan_Features.pdf"

@st.cache_resource
def load_vectorstore():
    if not os.path.exists(doc_path):
        st.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {doc_path} ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        st.stop()
    
    with st.spinner("üìö ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Database..."):
        loader = UnstructuredFileLoader(doc_path)
        docs_raw = loader.load()

        docs = []
        for d in docs_raw:
            source = d.metadata.get("source", doc_path)
            page_num = d.metadata.get("page_number", 1)
            docs.append(Document(page_content=d.page_content, metadata={"source": source, "page_number": page_num}))
        
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = splitter.split_documents(docs)
        logging.info(f"‚úÖ Document split into {len(chunks)} chunks")
        
        embed = OllamaEmbeddings(model="bge-m3", base_url=OLLAMA_URL)
        persist_dir = "chroma_db_pdf"
        
        if os.path.exists(persist_dir):
            vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embed)
            logging.info("üìÇ Loaded existing ChromaDB")
        else:
            vectorstore = Chroma.from_documents(chunks, embed, persist_directory=persist_dir)
            logging.info("üÜï Created new ChromaDB and persisted")
        
        return vectorstore

vectorstore = load_vectorstore()
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# ---------------------- LLM Setup ----------------------
llm = OllamaLLM(model="llama3.2:latest", base_url=OLLAMA_URL, temperature=0.2)

template = """
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡πÄ‡∏á‡∏¥‡∏ô ‡∏Å‡∏¢‡∏®.
‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£ ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

‡∏ö‡∏£‡∏¥‡∏ö‡∏ó (Context): {context}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (Question): {question}

‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö (Answer):
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
- ‡πÄ‡∏à‡∏≤‡∏∞‡∏à‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏° ‡∏Å‡∏¢‡∏®.
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"
"""
prompt = PromptTemplate(template=template, input_variables=["context", "question"])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt},
    return_source_documents=True
)

# ---------------------- Generate Answer ----------------------
def generate_answer(question: str):
    start_time = time.time()
    result = qa_chain({"query": question})
    end_time = time.time()

    answer_text = result["result"]
    retrieved_docs = result.get("source_documents", [])

    prompt_tokens = count_tokens(question)
    response_tokens = count_tokens(answer_text)
    response_time = end_time - start_time

    return answer_text, retrieved_docs, prompt_tokens, response_tokens, response_time

# ---------------------- Chat Interface ----------------------
if not st.session_state.get("chat_ended", False):
    if not st.session_state.messages:
        st.markdown("""
            <div class="info-box">
                <h3>üëã ‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏Å‡∏¢‡∏®</h3>
                <p>‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡πÄ‡∏á‡∏¥‡∏ô ‡∏Å‡∏¢‡∏® ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö</p>
                <p><strong>‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:</strong></p>
                <ul>
                    <li>‡∏â‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏Å‡∏π‡πâ ‡∏Å‡∏¢‡∏® ‡πÑ‡∏î‡πâ?</li>
                    <li>‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà?</li>
                    <li>‡∏â‡∏±‡∏ô‡∏à‡∏ö ‡∏°.6 ‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏π‡πâ ‡∏Å‡∏¢‡∏® ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°?</li>
                </ul>
            </div>
        """, unsafe_allow_html=True)

# ### >> FIX << ### ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÅ‡∏ä‡∏ó‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢
for msg in st.session_state.messages:
    avatar = "üë§" if msg["role"] == "user" else "ü§ñ"
    with st.chat_message(msg["role"], avatar=avatar):
        st.markdown(msg["content"])
        if msg["role"] == "assistant" and "pages" in msg:
            st.caption(f"üìÑ ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤: {msg['pages']}")

if st.session_state.messages and not st.session_state.get("chat_ended", False):
    st.markdown("---")
    col1, col2, col3 = st.columns([1, 1, 1])
    with col2:
        if st.button("üõë ‡∏à‡∏ö‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à", use_container_width=True):
            st.session_state.chat_ended = True
            st.rerun()

if not st.session_state.get("chat_ended", False):
    user_input = st.chat_input("üí¨ ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")
    
    if user_input:
        with st.chat_message("user", avatar="üë§"):
            st.markdown(user_input)
        
        with st.chat_message("assistant", avatar="ü§ñ"):
            with st.spinner("üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•..."):
                answer, retrieved_docs, prompt_tokens, response_tokens, response_time = generate_answer(user_input)
            
            st.markdown(answer)

            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            page_numbers_str = ""
            if retrieved_docs:
                unique_pages = sorted(list(set(doc.metadata.get("page_number", "N/A") for doc in retrieved_docs)))
                page_numbers_str = ", ".join(map(str, unique_pages))
                st.caption(f"üìÑ ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤: {page_numbers_str}")
            
            with st.expander("üìä ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"):
                col1, col2, col3 = st.columns(3)
                col1.metric("Prompt Tokens", prompt_tokens)
                col2.metric("Response Tokens", response_tokens)
                col3.metric("Response Time", f"{response_time:.2f}s")
    
        # ### >> FIX << ### ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å session state ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢
        # Save to database
        user_message_id = save_user_message(user_input, answer)
        st.session_state.messages.append({"role": "user", "content": user_input, "id": user_message_id})
        save_retrieved_chunks(user_message_id, retrieved_docs)
        save_llm_metrics(user_message_id, prompt_tokens, response_tokens, response_time)

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á assistant ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏•‡∏Ç‡∏´‡∏ô‡πâ‡∏≤
        assistant_message = {"role": "assistant", "content": answer}
        if page_numbers_str:
            assistant_message["pages"] = page_numbers_str
        st.session_state.messages.append(assistant_message)
        
        st.rerun() 

# ---------------------- Feedback Section ----------------------
if st.session_state.get("chat_ended", False):
    st.markdown("""
        <div class="feedback-section">
            <h2 style="text-align: center; color: #3b82f6;">üí¨ ‡πÅ‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à</h2>
            <p style="text-align: center; color: #6b7280;">‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏≤‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏î‡πâ</p>
        </div>
    """, unsafe_allow_html=True)
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    satisfaction = st.radio(
        "üåü ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?",
        ["‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å üëç", "‡∏û‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏î‡πâ", "‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ä‡πà‡∏ß‡∏¢ üëé"],
        horizontal=True
    )
    
    feedback_text = st.text_area(
        "üí≠ ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö):",
        placeholder="‡∏ö‡∏≠‡∏Å‡πÄ‡∏£‡∏≤‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏Ñ‡∏ß‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á...",
        height=100
    )
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üì© ‡∏™‡πà‡∏á‡∏ü‡∏µ‡∏î‡πÅ‡∏ö‡∏Ñ", type="primary", use_container_width=True):
            if satisfaction:
                last_user_msg_id = None
                for msg in reversed(st.session_state.messages):
                    if msg["role"] == "user":
                        last_user_msg_id = msg.get("id")
                        break
                
                if last_user_msg_id:
                    save_feedback(last_user_msg_id, satisfaction, feedback_text)
                    st.success("‚úÖ ‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì!")
                    time.sleep(2)
                    st.session_state.messages = []
                    st.session_state.chat_ended = False
                    st.rerun()
                else:
                     st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ü‡∏µ‡∏î‡πÅ‡∏ö‡∏Ñ")
            else:
                st.warning("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á")
    
    with col2:
        if st.button("üîÑ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏ä‡∏ó‡πÉ‡∏´‡∏°‡πà", use_container_width=True):
            st.session_state.messages = []
            st.session_state.chat_ended = False
            st.rerun()