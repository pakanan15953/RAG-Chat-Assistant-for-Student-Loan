import os
import torch
import streamlit as st
import logging
import sqlite3
from datetime import datetime
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from ollama import chat

# ---------------------- Logging Setup ----------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
device = "cuda" if torch.cuda.is_available() else "cpu"
logging.info(f"Using device: {device}")

# ---------------------- Database Setup ----------------------
def init_db():
    conn = sqlite3.connect("questions.db")
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS questions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            question TEXT NOT NULL,
            answer TEXT NOT NULL,
            timestamp TEXT NOT NULL
        )
    """)
    conn.commit()
    conn.close()
    logging.info("üì¶ Database initialized successfully.")

def save_question_to_db(question, answer):
    conn = sqlite3.connect("questions.db")
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO questions (question, answer, timestamp)
        VALUES (?, ?, ?)
    """, (question, answer, datetime.now().isoformat()))
    conn.commit()
    conn.close()
    logging.info("‚úÖ Saved question to database.")

# ---------------------- RAG Pipeline ----------------------
logging.info("üìÑ Loading document...")
loader = UnstructuredFileLoader("Loan_Features.docx")
docs = loader.load()

if not docs or not docs[0].page_content.strip():
    logging.error("‚ùå ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    st.error("‚ùå ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    st.stop()

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(docs)
logging.info(f"‚úÖ Document split into {len(chunks)} chunks")

# Embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs={"device": device},
    encode_kwargs={"normalize_embeddings": True}
)
logging.info("üí° Embedding model loaded")

# Vector store
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="chroma_db"
)
logging.info("üìö Vector store created successfully")

# ---------------------- Retrieval with confidence ----------------------
def retrieve(query: str):
    docs_with_score = vectorstore.similarity_search_with_score(query, k=5)
    
    # ‡πÅ‡∏õ‡∏•‡∏á score ‡πÄ‡∏õ‡πá‡∏ô similarity (‡∏¢‡∏¥‡πà‡∏á‡πÉ‡∏Å‡∏•‡πâ 1 ‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô)
    filtered = [(doc, 1 - score) for doc, score in docs_with_score if score < 0.8]

    # top3
    top3 = filtered[:3]

    results = []
    for rank, (doc, similarity) in enumerate(top3, 1):
        # top1 confidence ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î, top2 ‡∏•‡∏î‡∏•‡∏á, top3 ‡∏•‡∏î‡∏•‡∏á‡∏≠‡∏µ‡∏Å
        confidence = similarity * 90 + (3 - rank) * 3  # top1+6, top2+3, top3+0
        results.append({"doc": doc, "confidence": confidence})
    
    return results

# ---------------------- Answer generation ----------------------
def generate_answer(query: str, context: str) -> str:
    messages = [
        {"role": "system", "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô ai ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡∏Å‡∏¢‡∏® ‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å context ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‡∏Ñ‡∏£‡∏±‡∏ö"},
        {"role": "user", "content": f"Context:\n{context}\n\nQ: {query}\nA:"}
    ]
    response = chat(model="llama3.2:latest", messages=messages)
    return response["message"]["content"]

# ---------------------- Streamlit UI ----------------------
st.set_page_config(page_title="RAG Chatbot ‡∏Å‡∏¢‡∏®", page_icon="üìÑ")
st.title("üìÑ RAG Chatbot ‡∏Å‡∏¢‡∏®")
st.write("‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ `Loan_Features.docx`")

# Initialize DB
init_db()

# User input
user_query = st.chat_input("‚ùì ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡∏Å‡∏¢‡∏®:")

if user_query:
    with st.spinner("üìö ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á..."):
        retrieved_docs = retrieve(user_query)
        context_parts = []
        for i, item in enumerate(retrieved_docs, 1):
            context_parts.append(f"{item['doc'].page_content} [Confidence: {item['confidence']:.1f}%]")
        context = "\n\n".join(context_parts)
        logging.info("üîç Retrieved relevant context with confidence")

    with st.spinner("üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
        answer = generate_answer(user_query, context)
        logging.info("‚úÖ Answer generated")

    # Show result
    st.markdown(f"**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {user_query}")
    st.markdown("**‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:**")
    st.markdown(answer)

    # Save to DB
    save_question_to_db(user_query, answer)
