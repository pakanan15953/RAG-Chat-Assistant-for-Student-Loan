import os
import torch
import streamlit as st
import logging
import sqlite3
from datetime import datetime
from langchain_community.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from ollama import chat

# ---------------------- Logging Setup ----------------------
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ---------------------- Database Setup ----------------------
def init_db():
    conn = sqlite3.connect("questions.db")
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS questions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            question TEXT NOT NULL,
            answer TEXT NOT NULL,
            timestamp TEXT NOT NULL
        )
    """)
    conn.commit()
    conn.close()
    logging.info("üì¶ Database initialized successfully.")

def save_question_to_db(question, answer):
    conn = sqlite3.connect("questions.db")
    cursor = conn.cursor()
    cursor.execute("""
        INSERT INTO questions (question, answer, timestamp)
        VALUES (?, ?, ?)
    """, (question, answer, datetime.now().isoformat()))
    conn.commit()
    conn.close()
    logging.info("‚úÖ Saved question to database.")

# ---------------------- RAG Pipeline ----------------------
# 1. Load document
logging.info("üìÑ Loading document...")
loader = UnstructuredFileLoader("Loan_Features.docx")
docs = loader.load()

# Check if document loaded properly
if not docs or not docs[0].page_content.strip():
    logging.error("‚ùå ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    st.error("‚ùå ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    st.stop()

# 2. Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = splitter.split_documents(docs)
logging.info(f"‚úÖ Document split into {len(chunks)} chunks")

# 3. Use HuggingFace multilingual embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs={"device": "cpu"},  # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ CPU ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    encode_kwargs={"normalize_embeddings": True}
)
logging.info("üí° Embedding model loaded")

# 4. Create vector store
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="chroma_db"
)
logging.info("üìö Vector store created successfully")

# 5. Retrieval function
def retrieve(query: str):
    return vectorstore.similarity_search(query, k=3)

# 6. Answer generation with Ollama
def generate_answer(query: str, context: str) -> str:
    messages = [
        {
            "role": "system",
            "content": "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô ai ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡∏Å‡∏¢‡∏® ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ ‡∏Ñ‡∏£‡∏±‡∏ö "
        },
        {
        "role": "user",
        "content": (
            "Context:\n"
            f"{context}\n\n"
            "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°-‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:\n"
            "Q: ‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà‡∏ï‡πà‡∏≠‡∏õ‡∏µ?\n"
            "A: ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 360,000 ‡∏ö‡∏≤‡∏ó‡∏ï‡πà‡∏≠‡∏õ‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö\n"
            "Q: ‡∏Ñ‡∏ô‡∏≠‡∏≤‡∏¢‡∏∏ 33 ‡∏õ‡∏µ ‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏´‡∏£‡∏∑‡∏≠‡πÑa‡∏°‡πà?\n"
            "A: ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏µ‡πà 4 (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏≠‡∏≤‡∏¢‡∏∏‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 35 ‡∏õ‡∏µ)\n\n"
            f"Q: {user_query}\n"
            "A:"
        )
    }
    ]
    response = chat(model="llama3.2:latest", messages=messages)
    return response["message"]["content"]

# ---------------------- Streamlit UI ----------------------
st.set_page_config(page_title="RAG Chatbot ‡∏Å‡∏¢‡∏®", page_icon="üìÑ")
st.title("üìÑ RAG Chatbot ‡∏Å‡∏¢‡∏®")
st.write("‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ `Loan_Features.docx`")

# Initialize DB
init_db()

# User input
user_query = st.chat_input("‚ùì ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏ú‡∏π‡πâ‡∏Å‡∏π‡πâ‡∏¢‡∏∑‡∏°‡∏Å‡∏¢‡∏®:")

if user_query:
    with st.spinner("üìö ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á..."):
        retrieved_docs = retrieve(user_query)
        context = "\n\n".join([doc.page_content for doc in retrieved_docs])
        logging.info("üîç Retrieved relevant context")

    with st.spinner("üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
        answer = generate_answer(user_query, context)
        logging.info("‚úÖ Answer generated")

    # Show result
    st.markdown(user_query)
    st.markdown(" ‚úÖ ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö")
    st.markdown(answer)

    # Save to DB
    save_question_to_db(user_query, answer)


